{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85b943a2-aa16-46af-954b-e7b85ee44e73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install gitpython tqdm databricks-langchain\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "643e777e-57ba-40f7-85e4-d684a77bfdea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad0b9791-aee5-4d46-9f42-89504783a55c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "from git import Repo\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import json\n",
    "import ast\n",
    "from pyspark.sql.functions import lit\n",
    "from databricks_langchain import ChatDatabricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cb3e837-a696-4f8d-b175-6511eba4405a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## User Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3697a42-67b9-4f0f-9dce-4d2173d56ea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# GITHUB_REPO_URL = \"https://github.com/birbalin25/CodeCritic.git\" \n",
    "GITHUB_REPO_URL = \"https://github.com/birbalin25/birnew-mlflow-export-import.git\"\n",
    "# check_directory = \"mlflow_export_import/bulk\" ### USER INPUT\n",
    "check_directory = \"\"\n",
    "DATABRICKS_MODEL_ENDPOINT = \"databricks-claude-3-7-sonnet\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61ee00f1-e92a-4994-b9f3-ddb8e43ae908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Set credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f622de97-7779-403c-8231-9060a78895ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "host = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get('browserHostName').getOrElse(None)\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63878a7f-c0b9-4148-a85e-ded8c6fd0d17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##LLM initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44963132-cd4d-4734-b2de-141ba59a53b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "llm = ChatDatabricks(model=DATABRICKS_MODEL_ENDPOINT, temperature=0, host=host, token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdeb0a8b-fc85-4cd4-a2a3-355b2133b45f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4c8954c-f271-4134-804a-b401cb1f6c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clone_repo(github_url):\n",
    "    print(f\"Cloning repo: {github_url} ...\")\n",
    "    temp_dir = tempfile.mkdtemp(prefix=\"repo_\")\n",
    "    Repo.clone_from(github_url, temp_dir)\n",
    "    return temp_dir\n",
    "\n",
    "def get_python_files(base_path):\n",
    "    excluded_files = {\"setup.py\", \"__init__.py\", \"_README.py\"}\n",
    "    python_files = []\n",
    "\n",
    "    for root, _, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            if not file.endswith(\".py\") and not file.endswith(\".ipynb\"):\n",
    "                continue\n",
    "\n",
    "            lower_file = file.lower()\n",
    "            if (file in excluded_files or lower_file.startswith(\"test\") or \"__pycache__\" in root):\n",
    "                continue\n",
    "\n",
    "            python_files.append(file_path)\n",
    "            \n",
    "    return python_files\n",
    "\n",
    "def analyze_file_with_langchain(file_content, file_name):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a senior performance engineer with deep expertise in Apache Spark, Python, Scala, and SQL. Your role is to analyze code for inefficiencies and provide precise, actionable optimization recommendations\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"Review the Python file `{file_name}`:\\n\"\n",
    "                f\"---BEGIN FILE CONTENT---\\n{file_content}\\n---END FILE CONTENT---\\n\"\n",
    "                \"\"\"\n",
    "                Analyze the given code for inefficiencies and provide clear, concise improvement suggestions.\n",
    "                - If the code is not related to Spark, respond with one or two concise sentences summarizing its purpose.\n",
    "                - If the code is simple and requires no improvements, respond exactly with: Code is simple. No change needed.\n",
    "                - If the code is Spark-related, identify and explain any inefficiencies and provide recommendations for improvements.\n",
    "                - If Spark tables are joined, determine whether they use Spark SQL or DataFrame syntax.\n",
    "                - Extract and return the join columns in the following format:\n",
    "                    Join_columns_dictionary = {'table1': ['column1'], 'table2': ['column1']}\n",
    "                - Use the actual table  names as keys (not dataframe names), and list the columns used in the join as values.\n",
    "                - If there are multiple joins, include all relevant tables and columns.\n",
    "                - If no table joins are present, respond with: No table joins are present in this code.\n",
    "                \"\"\"\n",
    "            ),\n",
    "        },\n",
    "    ]    \n",
    "    response = llm.invoke(messages)\n",
    "    return response[\"content\"] if \"content\" in response else str(response)\n",
    "\n",
    "def read_py(py_file):\n",
    "    with open(py_file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        file_content = f.read() \n",
    "        return file_content\n",
    "\n",
    "\n",
    "def read_ipynb(py_file):\n",
    "    with open(py_file, 'r', encoding='utf-8') as f:\n",
    "        notebook = json.load(f)\n",
    "\n",
    "    code_lines = []\n",
    "    for cell in notebook.get('cells', []):\n",
    "        if cell.get('cell_type') == 'code':\n",
    "            lines = cell.get('source', [])\n",
    "            code_lines.extend(lines)\n",
    "            code_lines.append('\\n')\n",
    "    file_content = ''.join(code_lines)      \n",
    "\n",
    "    return file_content\n",
    "\n",
    "\n",
    "def main():\n",
    "    responses = []\n",
    "\n",
    "    try:\n",
    "        repo_path = clone_repo(GITHUB_REPO_URL)\n",
    "        repo_path = os.path.join(repo_path, check_directory)\n",
    "        print(f\"Repo path: {repo_path}\")\n",
    "\n",
    "        python_files = get_python_files(repo_path)\n",
    "        print(f\"Found {len(python_files)} Python files to analyze.\\n\")\n",
    "\n",
    "        print(f\"python_files are {python_files}\")\n",
    "\n",
    "        for file in tqdm(python_files, desc=\"Analyzing Python files\"):\n",
    "            try:\n",
    "                if file.endswith('.py'):\n",
    "                    file_content = read_py(file)\n",
    "                if file.endswith('.ipynb'):\n",
    "                    file_content = read_ipynb(file)    \n",
    "\n",
    "                feedback = analyze_file_with_langchain(file_content, file)\n",
    "                responses.append({\"file\": file, \"feedback\": feedback})\n",
    "            except Exception as e:\n",
    "                responses.append({\"file\": file, \"feedback\": f\"[Error reading file]: {e}\"})\n",
    "        results = []\n",
    "        for item in responses:\n",
    "            file_path = item.get('file')\n",
    "            feedback = item.get('feedback', '')\n",
    "\n",
    "            content_match = re.search(r\"content=(['\\\"])(.*?)\\1\\s+additional_kwargs=\", feedback, re.DOTALL)\n",
    "            content_raw = content_match.group(2) if content_match else None\n",
    "\n",
    "            if content_raw:\n",
    "                unescaped_content = content_raw.encode('utf-8').decode('unicode_escape')\n",
    "            else:\n",
    "                unescaped_content = None\n",
    "\n",
    "            join_columns_dict = None\n",
    "            if unescaped_content:\n",
    "                join_match = re.search(r\"Join_columns_dictionary\\s*=\\s*({.*?})\", unescaped_content, re.DOTALL)\n",
    "                if join_match:\n",
    "                    try:\n",
    "                        join_columns_dict = ast.literal_eval(join_match.group(1))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to parse join_columns in {file_path}: {e}\")\n",
    "                        join_columns_dict = None\n",
    "\n",
    "            results.append({\n",
    "                'file': file_path,\n",
    "                'llm_feedback': unescaped_content,\n",
    "                'llm_feedback_raw': str(item),\n",
    "                'join_columns': str(join_columns_dict) if join_columns_dict else \"No table joins are present in this code.\"\n",
    "            })\n",
    "\n",
    "        df = spark.createDataFrame(results)\n",
    "\n",
    "        delta_table = \"bircatalog.birschema.llm_op\"\n",
    "        df = df.withColumn(\"llm_used\", lit(DATABRICKS_MODEL_ENDPOINT))\n",
    "\n",
    "        df.write.format(\"delta\").mode(\"append\").saveAsTable(delta_table)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    finally:\n",
    "        if os.path.exists(repo_path):\n",
    "            shutil.rmtree(repo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d88df021-e761-4cb6-874e-ab7d20b18f1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Invoke main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14045555-a604-470f-b306-c5d6b54c6ddc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2619941-ad5e-437b-a67a-1c7468dffc68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d475d043-9925-4399-873b-ab76ebd45ef6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM bircatalog.birschema.llm_op\n",
    "-- drop table bircatalog.birschema.llm_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "437d4724-231b-476c-bf67-4d24544edf88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8418158362761095,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "scanner",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
