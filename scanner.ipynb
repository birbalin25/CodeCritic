{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4c8954c-f271-4134-804a-b401cb1f6c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "from git import Repo\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "\n",
    "GITHUB_REPO_URL = \"https://<github>.git\"\n",
    "# DATABRICKS_MODEL_ENDPOINT = \"databricks-meta-llama-3-1-405b-instruct\" \n",
    "DATABRICKS_MODEL_ENDPOINT = \"databricks-claude-3-7-sonnet\" \n",
    "\n",
    "\n",
    "\n",
    "llm = ChatDatabricks(model=DATABRICKS_MODEL_ENDPOINT, temperature=0, host=\"https://<host>\", token=\"dapiXXX\")\n",
    "\n",
    "def clone_repo(github_url):\n",
    "    print(f\"\\nüì• Cloning repo: {github_url} ...\")\n",
    "    temp_dir = tempfile.mkdtemp(prefix=\"repo_\")\n",
    "    Repo.clone_from(github_url, temp_dir)\n",
    "    return temp_dir\n",
    "\n",
    "def get_python_files(base_path):\n",
    "    excluded_files = {\"setup.py\", \"__init__.py\", \"_README.py\"}\n",
    "    python_files = []\n",
    "\n",
    "    for root, _, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            if not file.endswith(\".py\"):\n",
    "                continue\n",
    "\n",
    "            lower_file = file.lower()\n",
    "            if (file in excluded_files or lower_file.startswith(\"test\") or \"__pycache__\" in root):\n",
    "                continue\n",
    "\n",
    "            python_files.append(file_path)\n",
    "\n",
    "    return python_files\n",
    "\n",
    "def analyze_file_with_langchain(file_content, file_name):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a senior performance engineer with deep expertise in Apache Spark, Python, Scala, and SQL. Your role is to analyze code for inefficiencies and provide precise, actionable optimization recommendations\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"Review the Python file `{file_name}`:\\n\"\n",
    "                f\"---BEGIN FILE CONTENT---\\n{file_content}\\n---END FILE CONTENT---\\n\"\n",
    "                \"Identify inefficiencies and suggest improvements, with a primary focus on Spark-related code. If the code is not Spark-related, conclude with one or two concise sentences. If the code is simple and needs no changes, respond with: 'Code is simple. No change needed'. Be clear and concise\"\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    return response[\"content\"] if \"content\" in response else str(response)\n",
    "\n",
    "def main():\n",
    "    responses = []\n",
    "\n",
    "    try:\n",
    "        repo_path = clone_repo(GITHUB_REPO_URL)\n",
    "\n",
    "        python_files = get_python_files(repo_path)\n",
    "        print(f\"\\nüîç Found {len(python_files)} Python files to analyze.\\n\")\n",
    "\n",
    "        python_files = python_files[:20]\n",
    "        print(f\"python_files izz {python_files}\")\n",
    "\n",
    "        for py_file in tqdm(python_files, desc=\"üîé Analyzing Python files\"):\n",
    "            try:\n",
    "                with open(py_file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                    file_content = f.read()\n",
    "\n",
    "                feedback = analyze_file_with_langchain(file_content, py_file)\n",
    "                responses.append({\"file\": py_file, \"feedback\": feedback})\n",
    "            except Exception as e:\n",
    "                responses.append({\"file\": py_file, \"feedback\": f\"[Error reading file]: {e}\"})\n",
    "\n",
    "        print(f\"type of responses is {type(responses)}\")\n",
    "        print(f\"responses is {responses}\")\n",
    "\n",
    "        results = []\n",
    "        for item in responses:\n",
    "            file_path = item.get('file')\n",
    "            feedback = item.get('feedback', '')\n",
    "\n",
    "            match = re.search(r'content=([\"\\'])(.*?)\\1\\s+additional_kwargs=', feedback, re.DOTALL)\n",
    "            content = match.group(2) if match else None\n",
    "\n",
    "            results.append({'file': file_path, 'content': content, 'raw_data': str(item)})\n",
    "\n",
    "        df = spark.createDataFrame(results)\n",
    "\n",
    "        delta_table = \"bircatalog.birschema.llm_op\"\n",
    "        df = df.withColumn(\"llm_used\", lit(DATABRICKS_MODEL_ENDPOINT))\n",
    "\n",
    "        # df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(delta_table)\n",
    "        df.write.format(\"delta\").mode(\"append\").saveAsTable(delta_table)\n",
    "\n",
    "    finally:\n",
    "        if os.path.exists(repo_path):\n",
    "            shutil.rmtree(repo_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "scanner",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
