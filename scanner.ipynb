{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85b943a2-aa16-46af-954b-e7b85ee44e73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install gitpython tqdm databricks-langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac87893c-b8ee-4b52-9c11-e4c831a046f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4c8954c-f271-4134-804a-b401cb1f6c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "from git import Repo\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import json\n",
    "import ast\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "\n",
    "GITHUB_REPO_URL = \"https://github.com/birbalin25/CodeCritic.git\"\n",
    "# DATABRICKS_MODEL_ENDPOINT = \"databricks-meta-llama-3-1-405b-instruct\" \n",
    "DATABRICKS_MODEL_ENDPOINT = \"databricks-claude-3-7-sonnet\" \n",
    "\n",
    "\n",
    "\n",
    "llm = ChatDatabricks(model=DATABRICKS_MODEL_ENDPOINT, temperature=0, host=\"https:<host>\", token=\"dapiXXXX\")\n",
    "\n",
    "def clone_repo(github_url):\n",
    "    print(f\"\\nüì• Cloning repo: {github_url} ...\")\n",
    "    temp_dir = tempfile.mkdtemp(prefix=\"repo_\")\n",
    "    Repo.clone_from(github_url, temp_dir)\n",
    "    return temp_dir\n",
    "\n",
    "def get_python_files(base_path):\n",
    "    excluded_files = {\"setup.py\", \"__init__.py\", \"_README.py\"}\n",
    "    python_files = []\n",
    "\n",
    "    for root, _, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            if not file.endswith(\".py\") and not file.endswith(\".ipynb\"):\n",
    "                continue\n",
    "\n",
    "            lower_file = file.lower()\n",
    "            if (file in excluded_files or lower_file.startswith(\"test\") or \"__pycache__\" in root):\n",
    "                continue\n",
    "\n",
    "            python_files.append(file_path)\n",
    "            \n",
    "    return python_files\n",
    "\n",
    "def analyze_file_with_langchain(file_content, file_name):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a senior performance engineer with deep expertise in Apache Spark, Python, Scala, and SQL. Your role is to analyze code for inefficiencies and provide precise, actionable optimization recommendations\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"Review the Python file `{file_name}`:\\n\"\n",
    "                f\"---BEGIN FILE CONTENT---\\n{file_content}\\n---END FILE CONTENT---\\n\"\n",
    "                \"\"\"\n",
    "                Analyze the given code for inefficiencies and provide clear, concise improvement suggestions.\n",
    "                - If the code is not related to Spark, respond with one or two concise sentences summarizing its purpose.\n",
    "                - If the code is simple and requires no improvements, respond exactly with: Code is simple. No change needed.\n",
    "                - If the code is Spark-related, identify and explain any inefficiencies and provide recommendations for improvements.\n",
    "                - If Spark tables are joined, determine whether they use Spark SQL or DataFrame syntax.\n",
    "                - Extract and return the join columns in the following format:\n",
    "                    Join_columns_dictionary = {'table1': ['column1'], 'table2': ['column1']}\n",
    "                - Use the actual table or alias names as keys, and list the columns used in the join as values.\n",
    "                - If there are multiple joins, include all relevant tables and columns.\n",
    "                - If no table joins are present, respond with: No table joins are present in this code.\n",
    "                \"\"\"\n",
    "            ),\n",
    "        },\n",
    "    ]    \n",
    "    response = llm.invoke(messages)\n",
    "    return response[\"content\"] if \"content\" in response else str(response)\n",
    "\n",
    "def read_py(py_file):\n",
    "    with open(py_file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        file_content = f.read() \n",
    "        # print(f\"py file_content is {file_content}\")\n",
    "        return file_content\n",
    "\n",
    "\n",
    "def read_ipynb(py_file):\n",
    "    with open(py_file, 'r', encoding='utf-8') as f:\n",
    "        notebook = json.load(f)\n",
    "\n",
    "    code_lines = []\n",
    "    for cell in notebook.get('cells', []):\n",
    "        if cell.get('cell_type') == 'code':\n",
    "            lines = cell.get('source', [])\n",
    "            code_lines.extend(lines)\n",
    "            code_lines.append('\\n')\n",
    "    file_content = ''.join(code_lines)\n",
    "    # print(f\"ipynb file_content is {file_content}\")         \n",
    "\n",
    "    return file_content\n",
    "\n",
    "\n",
    "def main():\n",
    "    responses = []\n",
    "\n",
    "    try:\n",
    "        repo_path = clone_repo(GITHUB_REPO_URL)\n",
    "\n",
    "        python_files = get_python_files(repo_path)\n",
    "        print(f\"\\nüîç Found {len(python_files)} Python files to analyze.\\n\")\n",
    "\n",
    "        python_files = python_files[:20]\n",
    "        print(f\"python_files izz {python_files}\")\n",
    "\n",
    "        for file in tqdm(python_files, desc=\"üîé Analyzing Python files\"):\n",
    "            try:\n",
    "                if file.endswith('.py'):\n",
    "                    file_content = read_py(file)\n",
    "                if file.endswith('.ipynb'):\n",
    "                    file_content = read_ipynb(file)    \n",
    "\n",
    "                feedback = analyze_file_with_langchain(file_content, file)\n",
    "                responses.append({\"file\": file, \"feedback\": feedback})\n",
    "            except Exception as e:\n",
    "                responses.append({\"file\": file, \"feedback\": f\"[Error reading file]: {e}\"})\n",
    "\n",
    "        print(f\"type of responses is {type(responses)}\")\n",
    "        print(f\"responses is {responses}\")\n",
    "\n",
    "        results = []\n",
    "        for item in responses:\n",
    "            file_path = item.get('file')\n",
    "            feedback = item.get('feedback', '')\n",
    "\n",
    "            match = re.search(r'content=([\"\\'])(.*?)\\1\\s+additional_kwargs=', feedback, re.DOTALL)\n",
    "            content = match.group(2) if match else None\n",
    "\n",
    "            results.append({'file': file_path, 'llm_feedback': content, 'llm_feedback_raw': str(item)})\n",
    "\n",
    "        df = spark.createDataFrame(results)\n",
    "\n",
    "        delta_table = \"bircatalog.birschema.llm_op\"\n",
    "        df = df.withColumn(\"llm_used\", lit(DATABRICKS_MODEL_ENDPOINT))\n",
    "\n",
    "        # df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(delta_table)\n",
    "        df.write.format(\"delta\").mode(\"append\").saveAsTable(delta_table)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    finally:\n",
    "        if os.path.exists(repo_path):\n",
    "            shutil.rmtree(repo_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f7754e-7922-4c42-8e7c-f2ac4979087a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- drop table bircatalog.birschema.llm_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52a7c3c9-a298-4037-8157-8b33b2698138",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753246999242}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  regexp_replace(llm_feedback, '\\\\\\\\n\\\\\\\\n', '\\n\\n') AS llm_feedback,llm_feedback_raw,file, llm_used\n",
    "FROM bircatalog.birschema.llm_op"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8418158362755434,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "scanner",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
