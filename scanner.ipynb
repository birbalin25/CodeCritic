{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85b943a2-aa16-46af-954b-e7b85ee44e73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install gitpython tqdm databricks-langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac87893c-b8ee-4b52-9c11-e4c831a046f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4c8954c-f271-4134-804a-b401cb1f6c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "from git import Repo\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import json\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "\n",
    "GITHUB_REPO_URL = \"https://github.com/birbalin25/CodeCritic.git\"\n",
    "# DATABRICKS_MODEL_ENDPOINT = \"databricks-meta-llama-3-1-405b-instruct\" \n",
    "DATABRICKS_MODEL_ENDPOINT = \"databricks-claude-3-7-sonnet\" \n",
    "\n",
    "\n",
    "\n",
    "llm = ChatDatabricks(model=DATABRICKS_MODEL_ENDPOINT, temperature=0, host=\"https:<host>\", token=\"dapixxxxx\")\n",
    "\n",
    "def clone_repo(github_url):\n",
    "    print(f\"\\nüì• Cloning repo: {github_url} ...\")\n",
    "    temp_dir = tempfile.mkdtemp(prefix=\"repo_\")\n",
    "    Repo.clone_from(github_url, temp_dir)\n",
    "    return temp_dir\n",
    "\n",
    "def get_python_files(base_path):\n",
    "    excluded_files = {\"setup.py\", \"__init__.py\", \"_README.py\"}\n",
    "    python_files = []\n",
    "\n",
    "    for root, _, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            if not file.endswith(\".py\") and not file.endswith(\".ipynb\"):\n",
    "                continue\n",
    "\n",
    "            lower_file = file.lower()\n",
    "            if (file in excluded_files or lower_file.startswith(\"test\") or \"__pycache__\" in root):\n",
    "                continue\n",
    "\n",
    "            python_files.append(file_path)\n",
    "            \n",
    "    return python_files\n",
    "\n",
    "def analyze_file_with_langchain(file_content, file_name):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a senior performance engineer with deep expertise in Apache Spark, Python, Scala, and SQL. Your role is to analyze code for inefficiencies and provide precise, actionable optimization recommendations\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"Review the Python file `{file_name}`:\\n\"\n",
    "                f\"---BEGIN FILE CONTENT---\\n{file_content}\\n---END FILE CONTENT---\\n\"\n",
    "                \"Identify inefficiencies and suggest improvements, with a primary focus on Spark-related code. If the code is not Spark-related, conclude with one or two concise sentences. If the code is simple and needs no changes, respond with: 'Code is simple. No change needed'. Be clear and concise. If it is SPARK code, check if the tables are joined with spark sql or dataframes syntax. If yes, list the joining column in the dict format where key should be table name and value should be list of columns that are used in join. For example. If table1 and table2 are joined on table1.id = table2.id, then the dict should be {'table1': ['id'], 'table2': ['id']}.\"\n",
    "            ),\n",
    "        },\n",
    "    ]    \n",
    "    response = llm.invoke(messages)\n",
    "    return response[\"content\"] if \"content\" in response else str(response)\n",
    "\n",
    "def read_py(py_file):\n",
    "    with open(py_file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        file_content = f.read() \n",
    "        # print(f\"py file_content is {file_content}\")\n",
    "        return file_content\n",
    "\n",
    "\n",
    "def read_ipynb(py_file):\n",
    "    with open(py_file, 'r', encoding='utf-8') as f:\n",
    "        notebook = json.load(f)\n",
    "\n",
    "    code_lines = []\n",
    "    for cell in notebook.get('cells', []):\n",
    "        if cell.get('cell_type') == 'code':\n",
    "            lines = cell.get('source', [])\n",
    "            code_lines.extend(lines)\n",
    "            code_lines.append('\\n')\n",
    "    file_content = ''.join(code_lines)\n",
    "    # print(f\"ipynb file_content is {file_content}\")         \n",
    "\n",
    "    return file_content\n",
    "\n",
    "\n",
    "def main():\n",
    "    responses = []\n",
    "\n",
    "    try:\n",
    "        repo_path = clone_repo(GITHUB_REPO_URL)\n",
    "\n",
    "        python_files = get_python_files(repo_path)\n",
    "        print(f\"\\nüîç Found {len(python_files)} Python files to analyze.\\n\")\n",
    "\n",
    "        python_files = python_files[:20]\n",
    "        print(f\"python_files izz {python_files}\")\n",
    "\n",
    "        for file in tqdm(python_files, desc=\"üîé Analyzing Python files\"):\n",
    "            try:\n",
    "                if file.endswith('.py'):\n",
    "                    file_content = read_py(file)\n",
    "                if file.endswith('.ipynb'):\n",
    "                    file_content = read_ipynb(file)    \n",
    "\n",
    "                feedback = analyze_file_with_langchain(file_content, file)\n",
    "                responses.append({\"file\": file, \"feedback\": feedback})\n",
    "            except Exception as e:\n",
    "                responses.append({\"file\": file, \"feedback\": f\"[Error reading file]: {e}\"})\n",
    "\n",
    "        print(f\"type of responses is {type(responses)}\")\n",
    "        print(f\"responses is {responses}\")\n",
    "\n",
    "        results = []\n",
    "        for item in responses:\n",
    "            file_path = item.get('file')\n",
    "            feedback = item.get('feedback', '')\n",
    "\n",
    "            match = re.search(r'content=([\"\\'])(.*?)\\1\\s+additional_kwargs=', feedback, re.DOTALL)\n",
    "            content = match.group(2) if match else None\n",
    "\n",
    "            results.append({'file': file_path, 'content': content, 'raw_data': str(item)})\n",
    "\n",
    "        df = spark.createDataFrame(results)\n",
    "\n",
    "        delta_table = \"bircatalog.birschema.llm_op\"\n",
    "        df = df.withColumn(\"llm_used\", lit(DATABRICKS_MODEL_ENDPOINT))\n",
    "\n",
    "        # df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(delta_table)\n",
    "        df.write.format(\"delta\").mode(\"append\").saveAsTable(delta_table)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    finally:\n",
    "        if os.path.exists(repo_path):\n",
    "            shutil.rmtree(repo_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f7754e-7922-4c42-8e7c-f2ac4979087a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- drop table bircatalog.birschema.llm_op\n",
    "select * from bircatalog.birschema.llm_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "896ab49d-d0ec-469b-a4e9-3da0225312a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8418158362755409,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "scanner",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
